modelname:  word_idf_tword005_a03_wmt14_full_b32_lr3
model: attention

# Data:
input_data_src: data/WMT14_FULL/en_src
input_data_trg: data/WMT14_FULL/fr_trg

batch_size: 31
valid_batch_size: 15
decode: greedy 
model: attention
max_epochs : 4
learning_rate : 5e-4
save_checkpoint_every : 2000
# encoder
num_layers_src : 3
rnn_size_src : 1024
max_src_length : 80

# decoder
num_layers_src : 3
rnn_size_trg : 1024
max_trg_length : 80

# Loss
loss_version: word
similarity_matrix : data/WMT14_FULL/glove_w15d300.sparse_sim
alpha_word : 0.3
tau_word: 0.05
promote_rarity : 1
rarity_matrix : data/WMT14_FULL/promote_rare.sparse_matrix
