modelname : importance_qhamming_pool1_tsent013_rbleu4_tsent07_a06_iwslt14
model: attention
decode : greedy

# Data:
input_data_src: data/IWSLT14/de_src
input_data_trg: data/IWSLT14/en_trg

# config of Mixer (Ranzato et al.)
batch_size: 32
valid_batch_size: 32

learning_rate : 0.001
learning_rate_decay_every: 10
learning_rate_decay_rate : 0.5
max_epochs : 40

dim_word_src : 128
rnn_size_src : 256
num_layers_src : 1

dim_word_trg : 128
rnn_size_trg : 256
num_layers_trg : 1

# loss:
loss_version : seq
stratify_reward : 0
alpha_sent : 0.6

reward : bleu4
tau_sent : 0.7

importance_sampler : hamming
tau_sent_q : 0.13
limited_vocab_sub : 1


