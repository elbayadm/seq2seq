modelname:  word_tword005_a04_wmt14_full_bl60
model: attention

# Data:
input_data_src: data/WMT14_FULL/en_src
input_data_trg: data/WMT14_FULL/fr_trg

batch_size: 60
valid_batch_size: 25
decode: greedy 
model: attention
max_epochs : 4
learning_rate : 1e-4
save_checkpoint_every : 2000
# encoder
num_layers_src : 3
rnn_size_src : 1024
max_src_length : 80

# decoder
num_layers_src : 3
rnn_size_trg : 1024
max_trg_length : 80

# Loss
loss_version: word
similarity_matrix : data/WMT14_FULL/glove_w15d300.sparse_sim
alpha_word : 0.4
tau_word: 0.05
promote_rarity : 0
rarity_matrix : data/WMT14_FULL/promote_rare.matrix
