modelname : importance_qhamming_pool1_tsent013_rbleu4_tsent05_a03_wmt14_full_lr1

# Data:
input_data_src: data/WMT14_FULL/en_src
input_data_trg: data/WMT14_FULL/fr_trg

batch_size: 32
valid_batch_size: 10
decode: greedy 
model: attention
max_epochs : 4

# encoder
num_layers_src : 3
rnn_size_src : 1024
max_src_length : 80

# decoder
num_layers_src : 3
rnn_size_trg : 1024
max_trg_length : 80

learning_rate : 5e-5

# loss:
loss_version : seq
stratify_reward : 0
alpha_sent : 0.3

reward : bleu4
tau_sent : 0.5

importance_sampler : hamming
tau_sent_q : 0.13
limited_vocab_sub : 1


