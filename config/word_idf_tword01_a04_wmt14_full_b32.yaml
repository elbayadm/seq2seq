modelname:  word_idf_tword01_a04_wmt14_full_b32
model: attention

# Data:
input_data_src: data/WMT14_FULL/en_src
input_data_trg: data/WMT14_FULL/fr_trg

batch_size: 31
valid_batch_size: 10
decode: greedy 
model: attention
max_epochs : 4
learning_rate : 1e-4

# encoder
num_layers_src : 3
rnn_size_src : 1024
max_src_length : 80

# decoder
num_layers_src : 3
rnn_size_trg : 1024
max_trg_length : 80

# Loss
loss_version: word
similarity_matrix : data/WMT14_FULL/glove_w15d300.sparse_sim
alpha_word : 0.4
tau_word: 0.1
promote_rarity : 1
rarity_matrix : data/WMT14_FULL/promote_rare.sparse_matrix
